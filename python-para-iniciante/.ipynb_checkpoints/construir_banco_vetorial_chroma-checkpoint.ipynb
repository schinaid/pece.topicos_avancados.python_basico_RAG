{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "295ce4ee-2ac2-498e-b6c7-2ce1562e6503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Using cached langchain-0.3.27-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting langchain_community\n",
      "  Using cached langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.12/site-packages (4.13.4)\n",
      "Collecting chromadb\n",
      "  Using cached chromadb-1.0.16-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
      "Collecting sentence-transformers\n",
      "  Using cached sentence_transformers-5.1.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting langchain-core<1.0.0,>=0.3.72 (from langchain)\n",
      "  Using cached langchain_core-0.3.74-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.9 (from langchain)\n",
      "  Using cached langchain_text_splitters-0.3.9-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting langsmith>=0.1.17 (from langchain)\n",
      "  Using cached langsmith-0.4.13-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/conda/lib/python3.12/site-packages (from langchain) (2.11.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.12/site-packages (from langchain) (2.0.41)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.12/site-packages (from langchain) (2.32.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.12/site-packages (from langchain) (6.0.2)\n",
      "Collecting tenacity!=8.4.0,<10.0.0,>=8.1.0 (from langchain-core<1.0.0,>=0.3.72->langchain)\n",
      "  Using cached tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /opt/conda/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.14.0)\n",
      "Requirement already satisfied: packaging>=23.2 in /opt/conda/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/conda/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/conda/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2025.6.15)\n",
      "Requirement already satisfied: greenlet>=1 in /opt/conda/lib/python3.12/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.12/site-packages (from langchain_community) (3.12.13)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
      "  Using cached pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
      "  Using cached httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /opt/conda/lib/python3.12/site-packages (from langchain_community) (2.3.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.1)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Using cached marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
      "  Using cached python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Using cached mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.12/site-packages (from beautifulsoup4) (2.7)\n",
      "Collecting build>=1.0.3 (from chromadb)\n",
      "  Using cached build-1.3.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting pybase64>=1.4.1 (from chromadb)\n",
      "  Using cached pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /opt/conda/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.35.0)\n",
      "Collecting posthog<6.0.0,>=2.4.0 (from chromadb)\n",
      "  Using cached posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
      "  Using cached onnxruntime-1.22.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
      "  Using cached opentelemetry_api-1.36.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_grpc-1.36.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
      "  Using cached opentelemetry_sdk-1.36.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting tokenizers>=0.13.2 (from chromadb)\n",
      "  Using cached tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /opt/conda/lib/python3.12/site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /opt/conda/lib/python3.12/site-packages (from chromadb) (4.67.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /opt/conda/lib/python3.12/site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /opt/conda/lib/python3.12/site-packages (from chromadb) (6.5.2)\n",
      "Collecting grpcio>=1.58.0 (from chromadb)\n",
      "  Using cached grpcio-1.74.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb)\n",
      "  Using cached bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
      "Collecting typer>=0.9.0 (from chromadb)\n",
      "  Using cached typer-0.16.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb)\n",
      "  Using cached kubernetes-33.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting mmh3>=4.0.1 (from chromadb)\n",
      "  Using cached mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (14 kB)\n",
      "Collecting orjson>=3.9.12 (from chromadb)\n",
      "  Using cached orjson-3.11.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (42 kB)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /opt/conda/lib/python3.12/site-packages (from chromadb) (0.28.1)\n",
      "Collecting rich>=10.11.0 (from chromadb)\n",
      "  Using cached rich-14.1.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in /opt/conda/lib/python3.12/site-packages (from chromadb) (4.24.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.2 in /opt/conda/lib/python3.12/site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.9.0.post0)\n",
      "Collecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb)\n",
      "  Using cached backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: distro>=1.5.0 in /opt/conda/lib/python3.12/site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Using cached transformers-4.55.0-py3-none-any.whl.metadata (39 kB)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers)\n",
      "  Using cached torch-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Collecting scikit-learn (from sentence-transformers)\n",
      "  Using cached scikit_learn-1.7.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Collecting scipy (from sentence-transformers)\n",
      "  Using cached scipy-1.16.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (61 kB)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Using cached huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.12/site-packages (from sentence-transformers) (11.3.0)\n",
      "Collecting filelock (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Using cached regex-2025.7.34-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Using cached safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached fsspec-2025.7.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached hf_xet-1.1.7-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (703 bytes)\n",
      "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
      "  Using cached pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/conda/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.12/site-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.12/site-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.12/site-packages (from jsonschema>=4.19.0->chromadb) (0.25.1)\n",
      "Collecting google-auth>=1.0.1 (from kubernetes>=28.1.0->chromadb)\n",
      "  Using cached google_auth-2.40.3-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Collecting requests-oauthlib (from kubernetes>=28.1.0->chromadb)\n",
      "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /opt/conda/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: durationpy>=0.7 in /opt/conda/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Using cached cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Using cached pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Using cached rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting pyasn1>=0.1.3 (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Using cached pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting requests-toolbelt>=1.0.0 (from langsmith>=0.1.17->langchain)\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /opt/conda/lib/python3.12/site-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: flatbuffers in /opt/conda/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
      "Collecting protobuf (from onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached protobuf-6.31.1-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Collecting sympy (from onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /opt/conda/lib/python3.12/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
      "Collecting googleapis-common-protos~=1.57 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Using cached googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.36.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_common-1.36.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.36.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Using cached opentelemetry_proto-1.36.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.57b0 (from opentelemetry-sdk>=1.2.0->chromadb)\n",
      "  Using cached opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->chromadb)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from rich>=10.11.0->chromadb) (2.19.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (80.9.0)\n",
      "Collecting networkx (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.9.90 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
      "Collecting nvidia-nccl-cu12==2.27.3 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.8.90 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.4.0 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached triton-3.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /home/jovyan/.local/lib/python3.12/site-packages (from typer>=0.9.0->chromadb) (8.2.1)\n",
      "Collecting shellingham>=1.3.0 (from typer>=0.9.0->chromadb)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Using cached httptools-0.6.4-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: uvloop>=0.15.1 in /opt/conda/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Using cached watchfiles-1.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: websockets>=10.4 in /opt/conda/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.12/site-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence-transformers)\n",
      "  Using cached joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Using cached langchain-0.3.27-py3-none-any.whl (1.0 MB)\n",
      "Using cached langchain_core-0.3.74-py3-none-any.whl (443 kB)\n",
      "Using cached langchain_text_splitters-0.3.9-py3-none-any.whl (33 kB)\n",
      "Using cached tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Using cached langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
      "Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Using cached httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
      "Using cached marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Using cached pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached chromadb-1.0.16-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.6 MB)\n",
      "Using cached posthog-5.4.0-py3-none-any.whl (105 kB)\n",
      "Using cached sentence_transformers-5.1.0-py3-none-any.whl (483 kB)\n",
      "Using cached transformers-4.55.0-py3-none-any.whl (11.3 MB)\n",
      "Using cached huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "Using cached hf_xet-1.1.7-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "Using cached tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Using cached backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Using cached bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
      "Using cached build-1.3.0-py3-none-any.whl (23 kB)\n",
      "Using cached fsspec-2025.7.0-py3-none-any.whl (199 kB)\n",
      "Using cached grpcio-1.74.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.2 MB)\n",
      "Using cached kubernetes-33.1.0-py2.py3-none-any.whl (1.9 MB)\n",
      "Using cached google_auth-2.40.3-py2.py3-none-any.whl (216 kB)\n",
      "Using cached cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Using cached rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Using cached langsmith-0.4.13-py3-none-any.whl (372 kB)\n",
      "Using cached mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (103 kB)\n",
      "Using cached mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Using cached onnxruntime-1.22.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.5 MB)\n",
      "Using cached opentelemetry_api-1.36.0-py3-none-any.whl (65 kB)\n",
      "Using cached opentelemetry_exporter_otlp_proto_grpc-1.36.0-py3-none-any.whl (18 kB)\n",
      "Using cached opentelemetry_exporter_otlp_proto_common-1.36.0-py3-none-any.whl (18 kB)\n",
      "Using cached opentelemetry_proto-1.36.0-py3-none-any.whl (72 kB)\n",
      "Using cached googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "Using cached opentelemetry_sdk-1.36.0-py3-none-any.whl (119 kB)\n",
      "Using cached opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl (201 kB)\n",
      "Using cached protobuf-6.31.1-cp39-abi3-manylinux2014_x86_64.whl (321 kB)\n",
      "Using cached orjson-3.11.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (131 kB)\n",
      "Using cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Using cached pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Using cached pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n",
      "Using cached python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Using cached regex-2025.7.34-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (801 kB)\n",
      "Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Using cached rich-14.1.0-py3-none-any.whl (243 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Using cached safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "Using cached torch-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl (887.9 MB)\n",
      "Using cached nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "Using cached nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
      "Using cached nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
      "Using cached nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
      "Using cached nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.4 MB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Using cached triton-3.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.6 MB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached typer-0.16.0-py3-none-any.whl (46 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Using cached httptools-0.6.4-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (510 kB)\n",
      "Using cached watchfiles-1.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (452 kB)\n",
      "Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Using cached networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "Using cached pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
      "Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Using cached scikit_learn-1.7.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.5 MB)\n",
      "Using cached joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Using cached scipy-1.16.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.2 MB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: triton, threadpoolctl, tenacity, sympy, shellingham, scipy, safetensors, regex, python-dotenv, pyproject_hooks, pybase64, pyasn1, protobuf, orjson, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, mypy-extensions, mmh3, mdurl, marshmallow, joblib, humanfriendly, httpx-sse, httptools, hf-xet, grpcio, fsspec, filelock, cachetools, bcrypt, backoff, watchfiles, typing-inspect, scikit-learn, rsa, requests-toolbelt, requests-oauthlib, pyasn1-modules, posthog, opentelemetry-proto, opentelemetry-api, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, markdown-it-py, huggingface-hub, googleapis-common-protos, coloredlogs, build, tokenizers, rich, pydantic-settings, opentelemetry-semantic-conventions, opentelemetry-exporter-otlp-proto-common, onnxruntime, nvidia-cusolver-cu12, langsmith, google-auth, dataclasses-json, typer, transformers, torch, opentelemetry-sdk, langchain-core, kubernetes, sentence-transformers, opentelemetry-exporter-otlp-proto-grpc, langchain-text-splitters, langchain, chromadb, langchain_community\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m79/79\u001b[0m [langchain_community]ommunity]ext-splitters]rpc]\n",
      "\u001b[1A\u001b[2KSuccessfully installed backoff-2.2.1 bcrypt-4.3.0 build-1.3.0 cachetools-5.5.2 chromadb-1.0.16 coloredlogs-15.0.1 dataclasses-json-0.6.7 filelock-3.18.0 fsspec-2025.7.0 google-auth-2.40.3 googleapis-common-protos-1.70.0 grpcio-1.74.0 hf-xet-1.1.7 httptools-0.6.4 httpx-sse-0.4.1 huggingface-hub-0.34.4 humanfriendly-10.0 joblib-1.5.1 kubernetes-33.1.0 langchain-0.3.27 langchain-core-0.3.74 langchain-text-splitters-0.3.9 langchain_community-0.3.27 langsmith-0.4.13 markdown-it-py-3.0.0 marshmallow-3.26.1 mdurl-0.1.2 mmh3-5.2.0 mypy-extensions-1.1.0 networkx-3.5 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-nccl-cu12-2.27.3 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.8.90 onnxruntime-1.22.1 opentelemetry-api-1.36.0 opentelemetry-exporter-otlp-proto-common-1.36.0 opentelemetry-exporter-otlp-proto-grpc-1.36.0 opentelemetry-proto-1.36.0 opentelemetry-sdk-1.36.0 opentelemetry-semantic-conventions-0.57b0 orjson-3.11.1 posthog-5.4.0 protobuf-6.31.1 pyasn1-0.6.1 pyasn1-modules-0.4.2 pybase64-1.4.2 pydantic-settings-2.10.1 pyproject_hooks-1.2.0 python-dotenv-1.1.1 regex-2025.7.34 requests-oauthlib-2.0.0 requests-toolbelt-1.0.0 rich-14.1.0 rsa-4.9.1 safetensors-0.6.2 scikit-learn-1.7.1 scipy-1.16.1 sentence-transformers-5.1.0 shellingham-1.5.4 sympy-1.14.0 tenacity-9.1.2 threadpoolctl-3.6.0 tokenizers-0.21.4 torch-2.8.0 transformers-4.55.0 triton-3.4.0 typer-0.16.0 typing-inspect-0.9.0 watchfiles-1.1.0\n",
      "Requirement already satisfied: lxml in /opt/conda/lib/python3.12/site-packages (6.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain langchain_community beautifulsoup4 chromadb sentence-transformers\n",
    "!pip install -U lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "806545e3-4c79-4085-b899-1223ebc700d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from langchain_community.document_loaders import DirectoryLoader, BSHTMLLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from typing import Any, List, Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9df8a09-7233-4f75-93c8-1162971d63d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents_from_directory(\n",
    "    path: str = \"./data/python-3.13-docs-html\",\n",
    "    glob: str = \"**/*.html\",\n",
    "    use_bshtml_loader: bool = True,\n",
    "    show_progress: bool = True,\n",
    "    use_multithreading: bool = True,\n",
    "    get_text_separator: str = \" \"\n",
    ") -> List[Any]:\n",
    "    \"\"\"\n",
    "    Carrega arquivos de um diret√≥rio e retorna uma lista de `Document` (LangChain),\n",
    "    com as mesmas configura√ß√µes usadas no notebook original.\n",
    "\n",
    "    Par√¢metros\n",
    "    ----------\n",
    "    path : str\n",
    "        Caminho para a pasta contendo os arquivos (ex.: \"./data/python-3.13-docs-html\").\n",
    "    glob : str\n",
    "        Padr√£o de busca dos arquivos (padr√£o: \"**/*.html\").\n",
    "    use_bshtml_loader : bool\n",
    "        Se True, usa `BSHTMLLoader` para extrair texto de HTML.\n",
    "    show_progress : bool\n",
    "        Mostra barra de progresso durante carregamento.\n",
    "    use_multithreading : bool\n",
    "        Ativa carregamento com m√∫ltiplas threads (mais r√°pido).\n",
    "    get_text_separator : str\n",
    "        Separador usado pelo `BSHTMLLoader` ao extrair texto.\n",
    "\n",
    "    Retorno\n",
    "    -------\n",
    "    List[Any]\n",
    "        Lista de objetos `Document` do LangChain.\n",
    "\n",
    "    Tipos\n",
    "    -----\n",
    "    - path: str\n",
    "    - glob: str\n",
    "    - use_bshtml_loader: bool\n",
    "    - show_progress: bool\n",
    "    - use_multithreading: bool\n",
    "    - get_text_separator: str\n",
    "    - retorno: list[Document]\n",
    "    \"\"\"\n",
    "    if use_bshtml_loader:\n",
    "        loader = DirectoryLoader(\n",
    "            path=path,\n",
    "            glob=glob,\n",
    "            loader_cls=BSHTMLLoader,\n",
    "            show_progress=show_progress,\n",
    "            use_multithreading=use_multithreading,\n",
    "            loader_kwargs={'get_text_separator': get_text_separator}\n",
    "        )\n",
    "    else:\n",
    "        loader = DirectoryLoader(\n",
    "            path=path,\n",
    "            glob=glob,\n",
    "            show_progress=show_progress,\n",
    "            use_multithreading=use_multithreading\n",
    "        )\n",
    "    docs = loader.load()\n",
    "    print(f\"üìÇ {len(docs)} documentos carregados de '{path}'\")\n",
    "    return docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01398814-aab1-4c07-84c7-f26ba59cb5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_documents(\n",
    "    documents: Sequence[Any],\n",
    "    chunk_size: int = 1000,\n",
    "    chunk_overlap: int = 200,\n",
    "    add_start_index: bool = True\n",
    ") -> List[Any]:\n",
    "    \"\"\"\n",
    "    Divide documentos em peda√ßos (chunks) para indexa√ß√£o/busca.\n",
    "\n",
    "    Par√¢metros\n",
    "    ----------\n",
    "    documents : Sequence[Document]\n",
    "        Lista de documentos `Document`.\n",
    "    chunk_size : int, opcional (padr√£o: 1000)\n",
    "        Tamanho de cada chunk em caracteres.\n",
    "    chunk_overlap : int, opcional (padr√£o: 200)\n",
    "        Sobreposi√ß√£o em caracteres entre chunks.\n",
    "    add_start_index : bool, opcional (padr√£o: True)\n",
    "        Se True, adiciona no metadado o √≠ndice inicial do chunk no texto original.\n",
    "\n",
    "    Retorno\n",
    "    -------\n",
    "    List[Document]\n",
    "        Lista de chunks resultantes.\n",
    "    \"\"\"\n",
    "    # --- FASE 2: PROCESSAMENTO E INDEXA√á√ÉO ---\n",
    "    print(\"2. Dividindo os textos em peda√ßos (chunks)...\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        add_start_index=add_start_index\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(list(documents))\n",
    "    print(f\"‚úÇÔ∏è {len(chunks)} chunks criados (tamanho={chunk_size}, overlap={chunk_overlap}, add_start_index={add_start_index})\")\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d840d0f-9d70-436e-a2e0-dc0294551314",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hf_embeddings(\n",
    "    model_name: str = \"intfloat/multilingual-e5-small\",\n",
    "    device: str = \"cpu\",\n",
    "    normalize_embeddings: bool = True,\n",
    "    **kwargs: Any,\n",
    ") -> HuggingFaceEmbeddings:\n",
    "    \"\"\"\n",
    "    Cria um objeto de embeddings do HuggingFace para vetoriza√ß√£o.\n",
    "\n",
    "    Par√¢metros\n",
    "    ----------\n",
    "    model_name : str, opcional (padr√£o: \"intfloat/multilingual-e5-small\")\n",
    "        Nome do modelo de embeddings (Hugging Face Hub).\n",
    "    device : str, opcional (padr√£o: \"cpu\")\n",
    "        Dispositivo de execu√ß√£o (\"cpu\", \"cuda\", etc.).\n",
    "    normalize_embeddings : bool, opcional (padr√£o: True)\n",
    "        Se True, normaliza os vetores (√∫til para similaridade de cosseno).\n",
    "    **kwargs : Any\n",
    "        Par√¢metros extras repassados (ex.: model_kwargs, encode_kwargs).\n",
    "\n",
    "    Retorno\n",
    "    -------\n",
    "    HuggingFaceEmbeddings\n",
    "        Inst√¢ncia configurada do wrapper de embeddings.\n",
    "\n",
    "    Tipos\n",
    "    -----\n",
    "    - model_name: str\n",
    "    - device: str\n",
    "    - normalize_embeddings: bool\n",
    "    - retorno: HuggingFaceEmbeddings\n",
    "    \"\"\"\n",
    "    model_kwargs = kwargs.pop(\"model_kwargs\", {\"device\": device})\n",
    "    encode_kwargs = kwargs.pop(\"encode_kwargs\", {\"normalize_embeddings\": normalize_embeddings})\n",
    "    return HuggingFaceEmbeddings(\n",
    "        model_name=model_name,\n",
    "        model_kwargs=model_kwargs,\n",
    "        encode_kwargs=encode_kwargs,\n",
    "        **kwargs,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45e9c92e-1075-461e-a8cb-499ae5b55329",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_persist_chroma_index(\n",
    "    documents: Sequence[Any],\n",
    "    embeddings: HuggingFaceEmbeddings,\n",
    "    persist_directory: str,\n",
    ") -> Chroma:\n",
    "    \"\"\"\n",
    "    Cria **e persiste** um √≠ndice Chroma local a partir de `documents` usando `embeddings`.\n",
    "\n",
    "    Par√¢metros\n",
    "    ----------\n",
    "    documents : Sequence[Document]\n",
    "        Documentos (idealmente j√° chunkados) a serem indexados.\n",
    "    embeddings : HuggingFaceEmbeddings\n",
    "        Objeto de embeddings HuggingFace para vetoriza√ß√£o.\n",
    "    persist_directory : str\n",
    "        Diret√≥rio onde o √≠ndice Chroma ser√° salvo (cria caso n√£o exista).\n",
    "\n",
    "    Retorno\n",
    "    -------\n",
    "    Chroma\n",
    "        Inst√¢ncia do vetor store criada e j√° persistida.\n",
    "\n",
    "    Efeitos Colaterais\n",
    "    ------------------\n",
    "    - Cria/atualiza arquivos do Chroma no `persist_directory` (ex.: sqlite + blobs).\n",
    "\n",
    "    Tipos\n",
    "    -----\n",
    "    - documents: Sequence[Document]\n",
    "    - embeddings: HuggingFaceEmbeddings\n",
    "    - persist_directory: str\n",
    "    - retorno: Chroma\n",
    "    \"\"\"\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=list(documents),\n",
    "        embedding=embeddings,\n",
    "        persist_directory=persist_directory,\n",
    "    )\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0352c802-7832-4f84-8f9f-d3e0c47b80e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 163/164 [00:24<00:00,  6.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ 163 documentos carregados de './data/python-3.13-docs-html/'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "TextSplitter.__init__() got an unexpected keyword argument 'start_index'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m PERSIST_DIR = \u001b[33m\"\u001b[39m\u001b[33mchroma_db_python_iniciante\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     11\u001b[39m docs = load_documents_from_directory(INPUT_DIR, glob=GLOB, use_bshtml_loader=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m chunks = \u001b[43msplit_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCHUNK_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_overlap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCHUNK_OVERLAP\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m emb = build_hf_embeddings(EMBEDDING_MODEL, device=DEVICE, normalize_embeddings=NORMALIZE)\n\u001b[32m     14\u001b[39m vs = build_and_persist_chroma_index(chunks, emb, persist_directory=PERSIST_DIR)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36msplit_documents\u001b[39m\u001b[34m(documents, chunk_size, chunk_overlap, start_index)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msplit_documents\u001b[39m(\n\u001b[32m      2\u001b[39m     documents: Sequence[Any],\n\u001b[32m      3\u001b[39m     chunk_size: \u001b[38;5;28mint\u001b[39m = \u001b[32m800\u001b[39m,\n\u001b[32m      4\u001b[39m     chunk_overlap: \u001b[38;5;28mint\u001b[39m = \u001b[32m80\u001b[39m,\n\u001b[32m      5\u001b[39m     start_index: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      6\u001b[39m ) -> List[Any]:\n\u001b[32m      7\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[33;03m    Divide documentos em chunks para indexa√ß√£o/busca com melhor controle de contexto.\u001b[39;00m\n\u001b[32m      9\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     29\u001b[39m \u001b[33;03m    - retorno: list[Document]\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     splitter = \u001b[43mRecursiveCharacterTextSplitter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunk_overlap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_overlap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstart_index\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m splitter.split_documents(\u001b[38;5;28mlist\u001b[39m(documents))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/langchain_text_splitters/character.py:94\u001b[39m, in \u001b[36mRecursiveCharacterTextSplitter.__init__\u001b[39m\u001b[34m(self, separators, keep_separator, is_separator_regex, **kwargs)\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m     87\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     88\u001b[39m     separators: Optional[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     91\u001b[39m     **kwargs: Any,\n\u001b[32m     92\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     93\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Create a new TextSplitter.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkeep_separator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_separator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28mself\u001b[39m._separators = separators \u001b[38;5;129;01mor\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     96\u001b[39m     \u001b[38;5;28mself\u001b[39m._is_separator_regex = is_separator_regex\n",
      "\u001b[31mTypeError\u001b[39m: TextSplitter.__init__() got an unexpected keyword argument 'start_index'"
     ]
    }
   ],
   "source": [
    "INPUT_DIR = \"./data/python-3.13-docs-html/\" \n",
    "GLOB = \"**/*.html\"\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 200\n",
    "start_index = True\n",
    "EMBEDDING_MODEL = \"intfloat/multilingual-e5-small\"\n",
    "DEVICE = \"cpu\"                            # ou \"cuda\"\n",
    "NORMALIZE = True\n",
    "PERSIST_DIR = \"chroma_db_python_iniciante\"\n",
    "\n",
    "docs = load_documents_from_directory(INPUT_DIR, glob=GLOB, use_bshtml_loader=True)\n",
    "chunks = split_documents(docs, chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "emb = build_hf_embeddings(EMBEDDING_MODEL, device=DEVICE, normalize_embeddings=NORMALIZE)\n",
    "vs = build_and_persist_chroma_index(chunks, emb, persist_directory=PERSIST_DIR)\n",
    "\n",
    "print(\"Banco vetorial criado e persistido em:\", PERSIST_DIR)\n",
    "print(\"Total de chunks indexados (aprox.):\", len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429fc860-69c0-4752-a0cf-78fb52112418",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7f7862-978e-43c6-8e3b-a873b633bc21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c814445b-9fa2-4f1f-bc75-939f9e3a6e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Certifique-se de que o caminho para a pasta descompactada est√° correto.\n",
    "loader = DirectoryLoader(\n",
    "    path='./data/python-3.13-docs-html', # O caminho para a pasta que voc√™ criou\n",
    "    glob=\"**/*.html\",          # Pega todos os arquivos .html em todas as subpastas\n",
    "    loader_cls=BSHTMLLoader,    # Usa o BeautifulSoup para ler os arquivos HTML\n",
    "    show_progress=True,         # Mostra uma barra de progresso (muito √∫til!)\n",
    "    use_multithreading=True,    # Acelera o carregamento dos arquivos\n",
    "    loader_kwargs={'get_text_separator': ' '} # Argumento para o BSHTMLLoader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ed10409-c1b3-4bc8-92f7-a7dd2a5b3bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 163/164 [00:23<00:00,  6.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Carregados 163 documentos.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "docs = loader.load()\n",
    "print(f\"‚úÖ Carregados {len(docs)} documentos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b10203e-cd11-423c-87fa-30e945d874d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Dividindo os textos em peda√ßos (chunks)...\n"
     ]
    }
   ],
   "source": [
    "# --- FASE 2: PROCESSAMENTO E INDEXA√á√ÉO ---\n",
    "print(\"2. Dividindo os textos em peda√ßos (chunks)...\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=200,\n",
    "    add_start_index=True # Adiciona o √≠ndice inicial para refer√™ncia\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fdb155c-0f08-402b-aef7-7fdd2fd7ff0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Textos divididos em 9293 chunks.\n"
     ]
    }
   ],
   "source": [
    "splits = text_splitter.split_documents(docs)\n",
    "print(f\"‚úÖ Textos divididos em {len(splits)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbf90ed1-0c44-4937-9934-0b9c5fa6e883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. Criando embeddings para todos os chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_55/868508868.py:5: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n",
      "/opt/conda/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "print(\"3. Criando embeddings para todos os chunks...\")\n",
    "model_name = \"intfloat/multilingual-e5-small\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "#modelos usados para testar BAAI/bge-base-en-v1.5, llama3.2:3b-instruct-q4_K_M, qwen2.5:1.5b-instruct-q4_K_M, phi3:mini e intfloat/multilingual-e5-small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c830e4d-6897-4880-b9ab-043e87121f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Banco de dados vetorial criado e salvo no disco!\n"
     ]
    }
   ],
   "source": [
    "# MUDAN√áA: Usar um diret√≥rio persistente para o ChromaDB\n",
    "# Isso salva o banco de dados no disco para que voc√™ n√£o precise recri√°-lo toda vez!\n",
    "persist_directory = 'chroma_db_python_iniciante'\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=splits, \n",
    "    embedding=embeddings,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "print(\"‚úÖ Banco de dados vetorial criado e salvo no disco!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dbe04637-d724-4583-878f-589715a2fdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- FASE 3: CONSTRUINDO A CADEIA DE RESPOSTAS (RAG) ---\n",
    "prompt_template = \"\"\"\n",
    "Use o contexto a seguir para responder √† pergunta no final.\n",
    "Se voc√™ n√£o sabe a resposta, apenas diga que n√£o sabe, n√£o tente inventar uma resposta.\n",
    "Responda em Portugu√™s do Brasil. lembre-se voc√™ √© um assistente de python para iniciantes ent√£o responda claramente, com diversos exemplos passo a passo.\n",
    "\n",
    "{context}\n",
    "\n",
    "Pergunta: {question}\n",
    "Resposta √∫til:\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44693326-db6d-458f-9a64-8f9ed1bf9ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_55/1003087283.py:1: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=\"llama3\", temperature=0)\n"
     ]
    }
   ],
   "source": [
    "llm = Ollama(model=\"llama3\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6f0cb89-36d7-4f82-9f36-f1c56ba6fc72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Assistente com a documenta√ß√£o completa pronto!\n"
     ]
    }
   ],
   "source": [
    "chain_type_kwargs = {\"prompt\": PROMPT}\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 5}), # Aumentamos para 5 chunks\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs=chain_type_kwargs\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Assistente com a documenta√ß√£o completa pronto!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd422e75-591b-43e8-a117-79f18a941e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perguntar_assistente(query):\n",
    "    print(f\"\\n[?] Pergunta: {query}\")\n",
    "    print(\"... Buscando na documenta√ß√£o completa e gerando a resposta...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    result = qa_chain.invoke({\"query\": query})\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"\\n[!] Resposta do Assistente (levou {end_time - start_time:.2f} segundos):\")\n",
    "    print(result[\"result\"])\n",
    "    print(\"\\n--- Fontes Utilizadas ---\")\n",
    "    # Mostra os nomes dos arquivos HTML usados para a resposta\n",
    "    sources = set([doc.metadata['source'] for doc in result['source_documents']])\n",
    "    for source in sources:\n",
    "        print(f\"- {source}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5eff24-dbbf-405e-a38f-52c2b87f996b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e075ed4f-6df6-4aa5-ad48-426bf84c48aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "perguntar_assistente(\"para que serve as classes?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00564bc3-4fb7-42b0-a55f-19490d612eb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
